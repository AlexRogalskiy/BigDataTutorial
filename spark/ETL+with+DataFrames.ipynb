{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ETL with DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace all \\*\\*\\*TODO\\*\\*\\* placeholders with proper Spark API function calls. \n",
    "#### You can use code completion with \"TAB\" to display the list of available functions\n",
    "#### Use Escape + H to see list of all the Jupyter commands and shortcuts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary functions and types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import HiveContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import BooleanType, StringType, IntegerType\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define UDF needed to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isDurationCorrect(duration):\n",
    "    return duration is not None and duration != 'INF' and 30 <= int(duration) and int(duration) <= 1200\n",
    "\n",
    "def removeBrackets(text):\n",
    "    return text[1:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register UDFs so they can be used in SQL statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "udfIsDurationCorrect=udf(isDurationCorrect, BooleanType())\n",
    "udfRemoveBrackets=udf(removeBrackets, StringType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "database = \"<your database here>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw dataset from Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logs = sqlContext.***TODO***(\"SELECT * FROM %s.logs\" % database)\n",
    "logs.show(5)\n",
    "logs.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave only SongPlayed events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "streams_raw = logs.***TODO***(logs['eventType'] == 'SongPlayed')\n",
    "streams_raw.show(5)\n",
    "streams_raw.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary 'eventType' column from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams_projected = streams_raw.***TODO***('eventType')\n",
    "streams_projected.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename 'itemId' column into 'trackId'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams = streams_projected.withColumnRenamed('itemId', 'trackId')\n",
    "streams.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exclude records with incorrect value in 'duration' field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams_correct = streams.***TODO***(udfIsDurationCorrect('duration'))\n",
    "streams_correct.show(5)\n",
    "streams_correct.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove brackets around 'ts' field and cast 'duration' field to integer type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams_cleaned = streams_correct.***TODO***(udfRemoveBrackets('ts').alias('ts'),\n",
    "                                             'host', 'userId', 'trackId', \n",
    "                                             streams_correct['duration'].cast('int'))\n",
    "streams_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove duplicates from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams_unique = streams_cleaned.***TODO***()\n",
    "streams_unique.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store cleaned dataset in Hive table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "streams_unique.write.***TODO***(\"%s.stream\" % database, mode=\"overwrite\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pySpark 1.6",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
